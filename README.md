# SentimentDetectionAI
This notebook will develop a model capable of identifying mental health problems based on posts published on Twitter, relying on different natural language preprocessing techniques and AI algorithms of different nature.t nature, in order to evaluate their performance and find the option with the best performance.

# Notebook: Sentiment Analysis of Mental Health Issues on Twitter

## Use Case:
This notebook explores sentiment analysis to identify mental health issues, such as anxiety, depression, or suicidal tendencies, from Twitter posts. By analyzing tweets, we aim to detect patterns that indicate mental health problems, potentially aiding in raising awareness or preventing severe outcomes.

## Objective:
The objective of this notebook is to:
Data Ingestion: Load and prepare the dataset containing Twitter posts.
Exploratory Data Analysis (EDA): Understand the dataset’s structure, including dimensions, missing values, and common words.
Data Cleaning: Remove unnecessary variables, handle outliers, and clean the text data by removing irrelevant characters and standardizing the format.
Preprocessing: Apply natural language processing techniques to prepare the text for analysis.
Dimensionality Reduction: Use techniques to reduce the high-dimensional data from the Bag of Words model to facilitate model training and visualization.

## Steps:

### Data Ingestion:
Download the dataset from a Google Drive link and load it into a Pandas DataFrame.
Exploratory Data Analysis (EDA):
Assess the dataset’s dimensions, sample data, variables, missing values, unique values, and visualize distributions with boxplots and histograms.
Analyze word frequency using a word cloud to identify commonly used words and their relevance, such as URLs, user mentions, and common terms.

### Data Cleaning:

#### Remove Unnecessary Variables:
Drop irrelevant columns such as Unnamed: 0, post_id, and others, keeping only post_text and label.

#### Handle Outliers:
Detect and remove outliers if present; however, with the removal of numeric columns, outlier detection is not applied.

#### Impute Missing Values:
Confirm that no missing values exist in the dataset, hence no imputation is needed.

#### Remove Emojis and User Mentions:
Eliminate emojis and mentions (@username) to focus on meaningful text content.

#### Remove Non-English Characters and URLs:
Filter out non-English characters and URLs to maintain the focus on textual analysis.

#### Remove Numbers and Punctuation:
Exclude numbers and punctuation to standardize the text content for analysis.

### Preprocessing:

#### Text Normalization:
Convert text to lowercase and remove extra spaces.

#### Lemmatization:
Reduce words to their base forms to unify variations.

#### Stop Words Removal:
Eliminate common stop words that do not contribute significant meaning.

#### Tokenization:
Break down text into individual words (tokens).

#### Reconstruct Text:
Rebuild the text from the cleaned and tokenized content.

### Dimensionality Reduction:

#### Bag of Words Model:
Convert text data into a matrix with 18,896 columns, representing the frequency of each word across 20,000 tweets.

#### Principal Component Analysis (PCA):
Apply PCA to transform the data into a new set of variables (principal components) to capture maximum variance with fewer dimensions.
Truncated Singular Value Decomposition (Truncated SVD):
Use Truncated SVD to reduce dimensions while preserving important features from the sparse matrix generated by the Bag of Words model.

### Training and Evaluation of AI Models
Training AI models involves adjusting the model’s parameters to minimize the error between its predictions and the actual values using a training dataset. Validation, on the other hand, evaluates how well the model generalizes to new data and is used to adjust hyperparameters and prevent overfitting.
To evaluate different models, the holdout method with 5-fold train-test partitions will be used, comparing their performance using the F1 metric, which combines precision and recall, and the confusion matrix to visualize correct and incorrect predictions.

#### Traditional Algorithms
Decision Tree: Builds a decision tree that visualizes the classification process.
K-Nearest Neighbors (KNN): Classifies based on the labels of the k nearest neighbors; it is simple but can be computationally expensive for large datasets.
Support Vector Classifier (SVC): Finds the optimal hyperplane to separate classes; effective in high dimensionality but can be slow in training.
These algorithms will be tuned using GridSearchCV to optimize hyperparameters.

#### Neural Network
Using Keras, a neural network is constructed to process texts. The network includes layers for vectorization, embedding, Transformer Encoder, Global Max Pooling, and dense layers. The loss function used is sparse_categorical_crossentropy, and the optimizer is Adam. Although it was trained for only two epochs due to time constraints, the model showed good performance, with an F1 Score of 0.75.

#### Transformers
TabPFN: Adapted for tabular data with a limit of 1024 entries. It uses undersample_balance to adjust the data.
Transtab: Also designed for tabular data with transformers, but was not evaluated due to environment execution issues.
Model Interpretability
Interpretability allows understanding and explaining a model’s predictions. However, it can be limited when dimensionality reduction techniques, such as PCA and tSVD, are applied, as new variables may not have a clear direct meaning.

## Conclusion:
The notebook provides a comprehensive approach to analyzing mental health issues from Twitter posts using sentiment analysis. It includes data ingestion, exploratory data analysis, data cleaning, text preprocessing, and dimensionality reduction. These steps prepare the dataset for further analysis and modeling, with a focus on reducing dimensionality to enhance the effectiveness of sentiment analysis algorithms.
TabPFN showed the worst performance (F1 ~ 0.58) due to its data size limitation.
Traditional Algorithms: Decision Tree (0.63), SVC (0.65), and KNN (0.65) had similar performances, with SVC being the slowest in training.
Neural Network: Achieved the best results (F1 ~ 0.75), and could potentially improve further with additional training epochs.
